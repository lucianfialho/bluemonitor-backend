#!/usr/bin/env python3
"""
Script para verificar t√≥picos e not√≠cias duplicadas no banco de dados.
"""
import asyncio
import sys
import os
from pathlib import Path
from typing import Dict, List, Set, Any
from collections import defaultdict

# Adiciona o diret√≥rio raiz ao PATH para garantir que os imports funcionem
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from app.core.database import mongodb_manager
from app.core.config import settings
from app.core.logging import configure_logging
import logging

# Configurar logging
configure_logging()
logger = logging.getLogger(__name__)

async def get_all_topics() -> List[Dict]:
    """Busca todos os t√≥picos no banco de dados."""
    async with mongodb_manager.get_db() as db:
        topics = await db.topics.find({"is_active": True}).to_list(length=None)
        return topics

async def get_articles_for_topic(topic_id: str) -> List[Dict]:
    """Busca todos os artigos de um t√≥pico espec√≠fico."""
    async with mongodb_manager.get_db() as db:
        topic = await db.topics.find_one({"_id": topic_id})
        if not topic:
            return []
            
        article_ids = topic.get("articles", [])
        if not article_ids:
            return []
            
        articles = await db.news.find({"_id": {"$in": article_ids}}).to_list(length=None)
        return articles

async def find_duplicate_articles() -> Dict[str, List[Dict]]:
    """Encontra artigos duplicados baseados na URL."""
    async with mongodb_manager.get_db() as db:
        # Agrupa artigos por URL
        pipeline = [
            {"$group": {
                "_id": "$url",
                "count": {"$sum": 1},
                "articles": {"$push": "$$ROOT"}
            }},
            {"$match": {"count": {"$gt": 1}}}
        ]
        
        duplicates = await db.news.aggregate(pipeline).to_list(length=None)
        return {d["_id"]: d["articles"] for d in duplicates}

async def check_topics():
    """Verifica t√≥picos e not√≠cias duplicadas."""
    logger.info("üîç Verificando t√≥picos e not√≠cias duplicadas...")
    
    try:
        # Buscar todos os t√≥picos
        topics = await get_all_topics()
        logger.info(f"üìä Total de t√≥picos ativos: {len(topics)}")
    except Exception as e:
        logger.error(f"Erro ao buscar t√≥picos: {str(e)}")
        return
    
    # Estat√≠sticas por t√≥pico
    topic_stats = []
    all_article_ids = set()
    duplicate_article_count = 0
    
    for topic in topics:
        topic_id = topic["_id"]
        articles = await get_articles_for_topic(topic_id)
        
        # Verificar artigos duplicados neste t√≥pico
        url_count = defaultdict(int)
        for article in articles:
            url_count[article.get('url', '')] += 1
        
        duplicate_urls = {url: count for url, count in url_count.items() if count > 1}
        
        # Estat√≠sticas
        stats = {
            "topic_id": str(topic_id),
            "title": topic.get("title", "Sem t√≠tulo"),
            "article_count": len(articles),
            "duplicate_urls": duplicate_urls,
            "duplicate_count": sum(1 for count in url_count.values() if count > 1)
        }
        topic_stats.append(stats)
        
        # Verificar se h√° artigos duplicados entre t√≥picos
        for article in articles:
            article_id = str(article["_id"])
            if article_id in all_article_ids:
                duplicate_article_count += 1
            all_article_ids.add(article_id)
    
    # Encontrar URLs duplicadas no banco de dados
    duplicate_urls = await find_duplicate_articles()
    
    # Exibir relat√≥rio
    logger.info("\nüìà ESTAT√çSTICAS DOS T√ìPICOS")
    logger.info("=" * 80)
    
    if not topic_stats:
        logger.warning("‚ö†Ô∏è  Nenhum t√≥pico ativo encontrado no banco de dados.")
        return
    
    for i, stats in enumerate(topic_stats, 1):
        logger.info(f"\nüìå T√ìPICO {i}/{len(topic_stats)}")
        logger.info(f"   T√≠tulo: {stats['title']}")
        logger.info(f"   ID: {stats['topic_id']}")
        logger.info(f"   üìö Total de artigos: {stats['article_count']}")
        
        # Mostrar os primeiros 3 artigos como amostra
        topic_articles = await get_articles_for_topic(stats['topic_id'])
        if topic_articles:
            logger.info("\n   üì∞ Amostra de artigos:")
            for j, article in enumerate(topic_articles[:3], 1):
                title = article.get('title', 'Sem t√≠tulo')
                source = article.get('source_name', 'Fonte desconhecida')
                date = article.get('publish_date', 'Data desconhecida')
                logger.info(f"      {j}. {title}")
                logger.info(f"         üìÖ {date} | üì∞ {source}")
        
        if stats['duplicate_count'] > 0:
            logger.warning(f"\n   ‚ö†Ô∏è  ATEN√á√ÉO: {stats['duplicate_count']} URLs duplicadas encontradas neste t√≥pico")
    
    # An√°lise de duplicatas
    logger.info("\nüîç AN√ÅLISE DE DUPLICATAS")
    logger.info("=" * 80)
    
    if duplicate_urls:
        logger.warning(f"‚ö†Ô∏è  ENCONTRADAS {len(duplicate_urls)} URLs DUPLICADAS NO BANCO DE DADOS")
        logger.warning("   Estas s√£o not√≠cias id√™nticas armazenadas m√∫ltiplas vezes.")
        
        # Mostrar apenas as 5 primeiras duplicatas para n√£o sobrecarregar o log
        for i, (url, articles) in enumerate(duplicate_urls.items(), 1):
            if i > 5:  # Limitar a 5 exemplos
                remaining = len(duplicate_urls) - 5
                logger.warning(f"\n   ...e mais {remaining} URLs duplicadas n√£o mostradas.")
                break
                
            logger.warning(f"\n   üîó URL DUPLICADA {i}:")
            logger.warning(f"      {url}")
            logger.warning(f"      Aparece em {len(articles)} documentos diferentes:")
            
            for j, article in enumerate(articles[:3], 1):  # Mostrar at√© 3 ocorr√™ncias
                title = article.get('title', 'Sem t√≠tulo')
                source = article.get('source_name', 'Fonte desconhecida')
                date = article.get('publish_date', 'Data desconhecida')
                logger.warning(f"      {j}. {title}")
                logger.warning(f"         üìÖ {date} | üì∞ {source} | üÜî {article['_id']}")
            
            if len(articles) > 3:
                logger.warning(f"      ...e mais {len(articles) - 3} ocorr√™ncias")
    else:
        logger.info("‚úÖ Nenhuma URL duplicada encontrada no banco de dados.")
    
    # Estat√≠sticas finais
    logger.info("\nüìä ESTAT√çSTICAS GERAIS")
    logger.info("=" * 80)
    logger.info(f"   üìÇ Total de t√≥picos ativos: {len(topic_stats)}")
    logger.info(f"   üì∞ Total de artigos √∫nicos em t√≥picos: {len(all_article_ids)}")
    logger.info(f"   üîÑ Artigos presentes em m√∫ltiplos t√≥picos: {duplicate_article_count}")
    
    # Recomenda√ß√µes
    logger.info("\nüí° RECOMENDA√á√ïES")
    logger.info("=" * 80)
    if duplicate_urls:
        logger.warning("   ‚ö†Ô∏è  Recomenda√ß√£o: Considere implementar uma limpeza de duplicatas")
        logger.warning("       para remover not√≠cias id√™nticas do banco de dados.")
    else:
        logger.info("   ‚úÖ O banco de dados est√° limpo, sem duplicatas detectadas.")
    
    if duplicate_article_count > 0:
        logger.warning(f"   ‚ö†Ô∏è  {duplicate_article_count} artigos aparecem em m√∫ltiplos t√≥picos.")
        logger.warning("       Verifique se esta duplica√ß√£o √© intencional.")

def main():
    """Fun√ß√£o principal para execu√ß√£o do script."""
    try:
        logger.info("üöÄ Iniciando verifica√ß√£o de t√≥picos e not√≠cias duplicadas...")
        asyncio.run(check_topics())
        logger.info("‚úÖ Verifica√ß√£o conclu√≠da com sucesso!")
    except Exception as e:
        logger.error(f"‚ùå Erro durante a execu√ß√£o do script: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
