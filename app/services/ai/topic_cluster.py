"""Topic clustering service for grouping related news articles."""
import logging
import re
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from datetime import datetime, timedelta
from bson import ObjectId
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity

from app.core.config import settings
from app.core.database import MongoDBManager
from app.services.ai.processor import ai_processor

logger = logging.getLogger(__name__)

from .fact_extraction import fact_extraction_system
from collections import Counter

class TopicCluster:
    """Service for clustering news articles into topics."""
    
    def __init__(self):
        """Initialize the topic clustering service."""
        self.min_samples = 1  # Permite clusters menores
        self.eps = 0.85  # Ajustado para melhor equil√≠brio
        self.min_topic_size = 1  # Permite t√≥picos com apenas 1 artigo
        self.max_topic_age_days = 30  # Per√≠odo maior para an√°lise
        self.max_articles_to_process = 1000  # Aumentado para incluir mais artigos
        self.similarity_threshold = 0.4  # Reduzido para agrupar t√≥picos mais diversos
        
        # Categorias pr√©-definidas para classifica√ß√£o
        self.categories = {
            'saude_tratamento': [
                'terapia ocupacional', 'fonoaudiologia', 'psic√≥logo infantil',
                'neuropediatra', 'interven√ß√£o precoce', 'tratamento TEA',
                'comorbidades', 'medica√ß√£o autismo', 'acompanhamento multidisciplinar',
                'sa√∫de mental', 'terapia ABA', 'integra√ß√£o sensorial',
                'tratamento para autismo', 'sa√∫de do autista', 'acompanhamento m√©dico',
                'terapia para autismo', 'interven√ß√£o terap√™utica', 'sa√∫de infantil',
                'desenvolvimento infantil', 'neurodesenvolvimento', 'sa√∫de neurol√≥gica',
                # Termos adicionais para medica√ß√µes e tratamentos
                'medicamento', 'medica√ß√£o', 'rem√©dio', 'f√°rmaco', 'droga',
                'aprova√ß√£o', 'aprovado', 'libera√ß√£o', 'liberado', 'autoriza√ß√£o', 'autorizado',
                'anvisa', 'ag√™ncia nacional de vigil√¢ncia sanit√°ria',
                'minist√©rio da sa√∫de', 'secretaria de sa√∫de',
                'estudo cl√≠nico', 'ensaio cl√≠nico', 'estudo de fase',
                'efic√°cia', 'eficiente', 'efetivo', 'benef√≠cio',
                'efeito colateral', 'efeito adverso', 'contraindica√ß√£o',
                'dose', 'dosagem', 'administra√ß√£o', 'prescri√ß√£o',
                'tratamento medicamentoso', 'terapia medicamentosa',
                'neurol√≥gico', 'neurologia', 'neurologista',
                'psiqui√°trico', 'psiquiatria', 'psiquiatra',
                'desenvolvimento neuropsicomotor', 'desenvolvimento cognitivo',
                'habilidades sociais', 'habilidades comunicativas',
                'transtorno de processamento sensorial', 'hipersensibilidade sensorial',
                'sintomas', 'manifesta√ß√µes', 'condi√ß√µes', 'comorbidade'
            ],
            'educacao_inclusiva': [
                'educa√ß√£o especial', 'sala de recursos', 'professor de apoio',
                'plano educacional individualizado', 'adapta√ß√£o curricular',
                'escola inclusiva', 'm√©todos de ensino', 'alfabetiza√ß√£o',
                'inclus√£o escolar', 'tecnologias educacionais',
                'ensino especial', 'atendimento educacional especializado',
                'educa√ß√£o inclusiva', 'pr√°ticas pedag√≥gicas', 'curr√≠culo adaptado',
                'projeto pedag√≥gico', 'ensino-aprendizagem', 'media√ß√£o escolar',
                'acessibilidade na educa√ß√£o', 'recursos pedag√≥gicos', 'forma√ß√£o de professores',
                # Termos adicionais para educa√ß√£o inclusiva
                'ensino adaptado', 'aprendizagem adaptada', 'estrat√©gias de ensino',
                'metodologia inclusiva', 'pedagogia inclusiva', 'did√°tica inclusiva',
                'escola regular', 'classe regular', 'turma regular', 'ensino regular',
                'material adaptado', 'material did√°tico adaptado', 'avalia√ß√£o adaptada',
                'comunica√ß√£o alternativa na escola', 'CAA na escola',
                'PECS na escola', 'm√©todo TEACCH', 'm√©todo ABA na escola',
                'apoio escolar', 'aux√≠lio escolar', 'acompanhamento escolar',
                'monitor escolar', 'tutor escolar', 'mediador escolar',
                'profissional de apoio', 'acompanhante especializado',
                'ambiente sensorial na escola', 'escola amiga do autista',
                'sala sensorial', 'espa√ßo sensorial', 'adapta√ß√£o sensorial',
                'inclus√£o social na escola', 'socializa√ß√£o na escola'
            ],
            'direitos_legislacao': [
                'lei berenice piana', 'estatuto da pessoa com defici√™ncia',
                'direitos trabalhistas', 'benef√≠cio assistencial', 'LOAS',
                'isen√ß√£o de impostos', 'direito √† educa√ß√£o', 'direitos autistas',
                'pol√≠ticas p√∫blicas', 'conselhos de direitos'
            ],
            'violencia_discriminacao': [
                # Termos gerais de viol√™ncia
                'bullying', 'agress√£o', 'agredid', 'viol√™ncia', 'maus-tratos', 'abuso', 
                'ass√©dio', 'xingamento', 'humilha√ß√£o', 'ofensa', 'amea√ßa', 'intimida√ß√£o',
                'persegui√ß√£o', 'preconceito', 'discrimina√ß√£o', 'hostilidade', 'ofensa',
                'constrangimento', 'opress√£o', 'coer√ß√£o',
                # Termos adicionais para discrimina√ß√£o sutil
                'exclus√£o', 'isolamento', 'segrega√ß√£o', 'separa√ß√£o',
                'neglig√™ncia', 'descaso', 'indiferen√ßa', 'falta de aten√ß√£o',
                'desrespeito', 'desprezo', 'ridiculariza√ß√£o', 'estigmatiza√ß√£o',
                'estere√≥tipo', 'estereotipa√ß√£o', 'r√≥tulo', 'rotula√ß√£o',
                'olhar diferente', 'tratar diferente', 'tratamento diferenciado',
                'barreira atitudinal', 'barreira social', 'microagress√£o',
                'n√£o aceita√ß√£o', 'n√£o inclus√£o', 'n√£o adapta√ß√£o',
                'inadequa√ß√£o', 'n√£o apropriado', 'comportamento inadequado',
                'n√£o √© normal', 'anormal', 'diferente dos outros',
                'falta de empatia', 'falta de compreens√£o', 'falta de sensibilidade',
                'desinforma√ß√£o', 'desconhecimento', 'ignor√¢ncia',
                'capacitismo', 'capacitista', 'preconceito sobre defici√™ncia',
                
                # Termos espec√≠ficos para viol√™ncia f√≠sica e psicol√≥gica
                'agress√£o f√≠sica', 'viol√™ncia f√≠sica', 'viol√™ncia psicol√≥gica', 
                'ass√©dio moral', 'viol√™ncia verbal', 'viol√™ncia institucional',
                'viola√ß√£o de direitos', 'direitos violados', 'direito violado',
                'v√≠tima de agress√£o', 'sofrendo agress√£o', 'sofrendo viol√™ncia',
                
                # Contexto escolar
                'viol√™ncia escolar', 'viol√™ncia na escola', 'agress√£o na escola',
                'bullying escolar', 'ass√©dio na escola', 'preconceito na escola',
                'discrimina√ß√£o na escola', 'viol√™ncia entre alunos', 'briga de alunos',
                'conflito escolar', 'agress√£o entre alunos',
                
                # Responsabiliza√ß√£o institucional
                'escola processada', 'processo contra escola', 'processo contra col√©gio',
                'responsabilidade da escola', 'responsabilidade do col√©gio',
                'a√ß√£o judicial contra escola', 'processo judicial', 'a√ß√£o judicial',
                'den√∫ncia contra escola', 'reclama√ß√£o contra escola', 'escola denunciada',
                'dire√ß√£o da escola', 'omiss√£o da escola', 'falha da escola', 'erro da escola',
                'responsabilidade civil', 'indeniza√ß√£o por danos', 'danos morais',
                'processo na justi√ßa', 'na justi√ßa', 'na vara da inf√¢ncia', 'MPE', 'Minist√©rio P√∫blico',
                'conselho tutelar', 'conselho de direitos', 'direitos humanos',
                'notifica√ß√£o extrajudicial', 'notifica√ß√£o √† escola', 'notifica√ß√£o ao col√©gio'
            ],
            'tecnologia_assistiva': [
                'comunica√ß√£o alternativa', 'CAA', 'aplicativo autismo',
                'software educacional', 'dispositivo adaptado', 'tecnologia inclusiva',
                'recursos de acessibilidade', 'comunica√ß√£o suplementar'
            ],
            'pesquisa_cientifica': [
                'estudo cient√≠fico', 'pesquisa autismo', 'neuroci√™ncia',
                'gen√©tica autismo', 'ensaios cl√≠nicos', 'artigo cient√≠fico',
                'descoberta cient√≠fica', 'pesquisa m√©dica'
            ],
            'familia_cuidadores': [
                'relato de m√£e', 'relato de pai', 'cuidadores', 'rede de apoio',
                'qualidade de vida familiar', 'desafios familiares', 'maternidade at√≠pica',
                'paternidade at√≠pica', 'grupo de apoio',
                # Termos adicionais para desafios familiares
                'pais', 'm√£es', 'respons√°veis', 'fam√≠lia', 'familiares', 'irm√£os',
                'desafio', 'dificuldade', 'obst√°culo', 'barreira', 'problema',
                'sobrecarga', 'estresse', 'burnout', 'esgotamento', 'exaust√£o',
                'rotina', 'dia a dia', 'cotidiano', 'conviv√™ncia', 'adapta√ß√£o',
                'apoio psicol√≥gico', 'apoio emocional', 'acolhimento',
                'suporte familiar', 'orienta√ß√£o familiar', 'aconselhamento',
                'rela√ß√£o familiar', 'din√¢mica familiar', 'ambiente familiar',
                'experi√™ncia familiar', 'viv√™ncia familiar', 'hist√≥ria familiar',
                'vida familiar', 'fam√≠lia at√≠pica', 'fam√≠lia neurodiversa',
                'impacto familiar', 'impacto na fam√≠lia', 'impacto no cuidador',
                'bem-estar familiar', 'bem-estar do cuidador', 'qualidade de vida',
                'cuidado parental', 'cria√ß√£o', 'educa√ß√£o em casa', 'educa√ß√£o familiar',
                'socializa√ß√£o', 'intera√ß√£o social', 'relacionamento interpessoal'
            ],
            'mercado_trabalho': [
                'inclus√£o profissional', 'empregabilidade', 'treinamento profissional',
                'empresas inclusivas', 'leis trabalhistas', 'qualifica√ß√£o profissional',
                'mercado de trabalho', 'carreira', 'oportunidades de emprego'
            ],
            'cultura_lazer': [
                'evento inclusivo', 'atividades recreativas', 'esportes adaptados',
                'oficinas culturais', 'teatro acess√≠vel', 'cinema inclusivo',
                'atividades l√∫dicas', 'lazer adaptado'
            ],
            'pesquisa_estatistica': [
                'pesquisa', 'estudo', 'levantamento', 'dados', 'estat√≠stica', 'censo',
                'pesquisadores', 'cientistas', 'universidade', 'institui√ß√£o de pesquisa',
                'IBGE', 'Instituto Brasileiro de Geografia e Estat√≠stica', 'dados oficiais',
                'relat√≥rio', 'an√°lise estat√≠stica', 'pesquisa cient√≠fica', 'estudo acad√™mico',
                'publica√ß√£o cient√≠fica', 'artigo cient√≠fico', 'revista cient√≠fica', 'peri√≥dico cient√≠fico',
                'metan√°lise', 'revis√£o sistem√°tica', 'estudo longitudinal', 'pesquisa de campo',
                'coleta de dados', 'an√°lise de dados', 'resultados de pesquisa', 'descoberta cient√≠fica',
                'inova√ß√£o em pesquisa', 'tecnologia assistiva', 'avan√ßo cient√≠fico', 'pesquisa cl√≠nica',
                'ensaio cl√≠nico', 'estudo multic√™ntrico', 'pesquisa translacional', 'pesquisa aplicada',
                'pesquisa b√°sica', 'pesquisa qualitativa', 'pesquisa quantitativa', 'm√©todos de pesquisa',
                'metodologia cient√≠fica', 'revis√£o por pares', 'fator de impacto', 'indexa√ß√£o em bases cient√≠ficas',
                'banco de dados de pesquisa', 'reposit√≥rio cient√≠fico', 'acesso aberto', 'ci√™ncia aberta',
                'divulga√ß√£o cient√≠fica', 'populariza√ß√£o da ci√™ncia', 'jornalismo cient√≠fico', 'comunica√ß√£o cient√≠fica',
                '√©tica em pesquisa', 'comit√™ de √©tica', 'comiss√£o nacional de √©tica em pesquisa', 'conep',
                'plataforma brasil', 'sistema cnpq', 'curr√≠culo lattes', 'plataforma lattes', 'diret√≥rio dos grupos de pesquisa',
                'dgp cnpq', 'grupos de pesquisa', 'linhas de pesquisa', 'projetos de pesquisa', 'bolsas de pesquisa',
                'inicia√ß√£o cient√≠fica', 'mestrado', 'doutorado', 'p√≥s-doutorado', 'produtividade em pesquisa',
                'pesquisador associado', 'pesquisador s√™nior', 'pesquisador visitante', 'colabora√ß√£o internacional',
                'coopera√ß√£o cient√≠fica', 'acordos de coopera√ß√£o', 'projetos conjuntos', 'redes de pesquisa',
                'associa√ß√µes cient√≠ficas', 'sociedades cient√≠ficas', 'congressos cient√≠ficos', 'eventos cient√≠ficos',
                'semanas acad√™micas', 'jornadas cient√≠ficas', 'semin√°rios', 'workshops', 'oficinas t√©cnicas',
                'cursos de capacita√ß√£o', 'treinamentos', 'palestras', 'mesas-redondas', 'debates', 'pain√©is',
                'apresenta√ß√µes orais', 'sess√µes de p√¥steres', 'resumos expandidos', 'anais de eventos',
                'proceedings', 'livros cient√≠ficos', 'cap√≠tulos de livros', 'colet√¢neas', 'edi√ß√µes especiais',
                'edi√ß√µes tem√°ticas', 'edi√ß√µes comemorativas', 'edi√ß√µes especiais', 'edi√ß√µes tem√°ticas',
                'edi√ß√µes comemorativas', 'edi√ß√µes especiais', 'edi√ß√µes tem√°ticas', 'edi√ß√µes comemorativas'
            ]
        }
        
        # Termos obrigat√≥rios para considerar um artigo relevante
        self.required_terms = [
            # Termos diretos sobre autismo
            'autis',  # Captura autismo, autista, autistas
            'TEA', 'transtorno do espectro autista',
            'neurodiversidade', 'neurodivergente',
            's√≠ndrome de asperger',
            'transtorno invasivo do desenvolvimento',
            'TID', 'TGD',
            'condi√ß√£o do espectro autista',
            'transtorno global do desenvolvimento',
            
            # Termos relacionados a defici√™ncia
            'crian√ßa especial', 'pessoa com defici√™ncia', 'PCD',
            'necessidades especiais', 'defici√™ncia intelectual',
            'transtorno do desenvolvimento', 'condi√ß√£o neurol√≥gica',
            'condi√ß√£o do neurodesenvolvimento', 'transtorno neurol√≥gico',
            'pessoa com defici√™ncia', 'pessoa com necessidades especiais',
            'crian√ßa com defici√™ncia', 'adolescente com defici√™ncia',
            'pessoa com transtorno', 'crian√ßa com transtorno',
            
            # Termos para capturar processos judiciais e responsabiliza√ß√£o
            'aluno com defici√™ncia', 'aluno especial', 'aluno autista',
            'crian√ßa autista', 'adolescente autista', 'pessoa autista',
            'estudante autista', 'estudante com defici√™ncia',
            'aluno com necessidades especiais', 'crian√ßa especial',
            'crian√ßa com necessidades especiais', 'adolescente com necessidades especiais',
            'pessoa com necessidades especiais', 'pessoa com TEA', 'pessoa com autismo',
            'crian√ßa com TEA', 'adolescente com TEA', 'estudante com TEA', 'aluno com TEA',
            
            # Termos adicionais para responsabiliza√ß√£o institucional
            'processo judicial', 'a√ß√£o judicial', 'processo na justi√ßa',
            'a√ß√£o na justi√ßa', 'a√ß√£o na vara da inf√¢ncia', 'a√ß√£o no MPE',
            'Minist√©rio P√∫blico Estadual', 'Minist√©rio P√∫blico Federal',
            'promotoria de justi√ßa', 'promotor de justi√ßa', 'promotora de justi√ßa',
            'defensoria p√∫blica', 'defensor p√∫blico', 'defensora p√∫blica',
            'vara da inf√¢ncia', 'vara da crian√ßa e do adolescente', 'juizado especial',
            'a√ß√£o civil p√∫blica', 'a√ß√£o de responsabilidade', 'a√ß√£o indenizat√≥ria',
            'a√ß√£o por danos morais', 'danos morais', 'danos materiais', 'danos est√©ticos',
            'indeniza√ß√£o por danos', 'indeniza√ß√£o por dano moral', 'indeniza√ß√£o por dano material',
            'responsabilidade civil', 'responsabilidade objetiva', 'responsabilidade subjetiva',
            'obriga√ß√£o de indenizar', 'dever de indenizar', 'dever de reparar',
            'repara√ß√£o de danos', 'repara√ß√£o civil', 'repara√ß√£o por danos',
            'responsabilidade da escola', 'responsabilidade do col√©gio',
            'responsabilidade da institui√ß√£o de ensino', 'responsabilidade do estabelecimento de ensino',
            'dever de cuidado', 'dever de vigil√¢ncia', 'dever de prote√ß√£o',
            'omiss√£o da escola', 'omiss√£o do col√©gio', 'falha na fiscaliza√ß√£o',
            'falha na supervis√£o', 'falha no acompanhamento', 'falha na seguran√ßa',
            'notifica√ß√£o extrajudicial', 'notifica√ß√£o √† escola', 'notifica√ß√£o ao col√©gio',
            'notifica√ß√£o √† dire√ß√£o', 'notifica√ß√£o √† secretaria', 'notifica√ß√£o ao conselho',
            'den√∫ncia contra escola', 'den√∫ncia ao conselho tutelar', 'den√∫ncia ao MP',
            'representa√ß√£o ao MP', 'representa√ß√£o ao Minist√©rio P√∫blico', 'queixa-crime',
            'boletim de ocorr√™ncia', 'B.O.', 'registro de ocorr√™ncia', 'termo circunstanciado',
            'delegacia de prote√ß√£o √† crian√ßa e ao adolescente', 'delegacia da crian√ßa e do adolescente',
            'conselho tutelar', 'conselho municipal dos direitos da crian√ßa e do adolescente',
            'conselho estadual dos direitos da crian√ßa e do adolescente', 'CMDCA', 'CEDCA',
            'v√≠tima de viol√™ncia', 'v√≠tima de agress√£o', 'v√≠tima de maus-tratos',
            'v√≠tima de bullying', 'v√≠tima de ass√©dio', 'v√≠tima de discrimina√ß√£o',
            'v√≠tima de preconceito', 'v√≠tima de neglig√™ncia', 'v√≠tima de omiss√£o',
            'v√≠tima de abandono intelectual', 'v√≠tima de abuso', 'v√≠tima de viola√ß√£o de direitos'
        ]
        
        # Palavras-chave para identificar not√≠cias irrelevantes
        self.irrelevant_keywords = [
            # Entretenimento e lazer
            'futebol', 'esporte', 'jogo', 'partida', 'campeonato', 'time', 'sele√ß√£o',
            'celebridade', 'famoso', 'famosos', 'famosas', 'ator', 'atriz', 'cantor', 'cantora',
            'entretenimento', 'novela', 's√©rie', 'filme', 'cinema', 'm√∫sica', 'show', 'festival',
            'bbb', 'big brother', 'reality show', 'programa de audit√≥rio', 'lazer', 'viagem',
            'turismo', 'passeio', 'passeios', 'feriado', 'f√©rias', 'hotel', 'resort',
            'culin√°ria', 'receita', 'comida', 'gastronomia', 'restaurante', 'chef', 'cozinha',
            'moda', 'beleza', 'maquiagem', 'cabelo', 'est√©tica', 'cosm√©tico', 'perfume',
            'automobilismo', 'corrida', 'f√≥rmula 1', 'f1', 'moto', 'carro', 'autom√≥vel',
            'pol√≠tica partid√°ria', 'elei√ß√µes', 'candidato', 'candidata', 'partido', 'governo',
            'religi√£o', 'igreja', 'templo', 'culto', 'missa', 'bispo', 'pastor', 'padre',
            'fofoca', 'celebridades', 'famosinhos', 'celebridade internacional', 'hollywood'
        ]
    
    def is_relevant(self, text: str) -> bool:
        """Verifica se o texto √© relevante para autismo."""
        if not text:
            return False
            
        text_lower = text.lower()
        
        # Verifica se cont√©m algum termo obrigat√≥rio
        has_required = any(term in text_lower for term in self.required_terms)
        
        # Verifica se cont√©m palavras irrelevantes
        has_irrelevant = any(term in text_lower for term in self.irrelevant_keywords)
        
        # Verifica se √© uma not√≠cia de pesquisa/estat√≠stica sobre autismo
        is_research = any(term in text_lower for term in self.categories['pesquisa_estatistica'])
        is_about_autism = any(term in text_lower for term in ['autis', 'TEA', 'transtorno do espectro autista'])
        
        # √â relevante se:
        # 1. Tem termos obrigat√≥rios E n√£o tem termos irrelevantes, OU
        # 2. √â uma not√≠cia de pesquisa/estat√≠stica sobre autismo
        return (has_required and not has_irrelevant) or (is_research and is_about_autism)
    
    async def _preprocess_topic_facts(self, db, topic_id: str, topic_name: str) -> None:
        """Pr√©-processa fatos para um t√≥pico espec√≠fico.
        
        Args:
            db: Conex√£o com banco de dados
            topic_id: ID do t√≥pico
            topic_name: Nome do t√≥pico
        """
        try:
            logger.info(f"üß† Pr√©-processando fatos para t√≥pico: {topic_name}")
            
            # Extrair fatos do t√≥pico
            facts = await fact_extraction_system.extract_facts_from_topic(db, topic_id)
            facts_summary = fact_extraction_system.get_facts_summary(facts)
            
            # Salvar fatos no t√≥pico
            await db.topics.update_one(
                {'_id': ObjectId(topic_id)},
                {
                    '$set': {
                        'extracted_facts': facts[:20],  # Top 20 fatos mais relevantes
                        'facts_summary': facts_summary,
                        'facts_processed': True,
                        'facts_processed_at': datetime.utcnow(),
                        'total_facts_available': len(facts)
                    }
                }
            )
            
            logger.info(f"‚úÖ {topic_name}: {len(facts)} fatos extra√≠dos, {len(facts[:20])} salvos")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao pr√©-processar fatos para {topic_name}: {str(e)}")
            # N√£o falha o clustering se a extra√ß√£o de fatos der erro

    def _categorize_article(self, article: Dict[str, Any]) -> str:
        """Categorize an article into one of the predefined categories.
        
        Args:
            article: Dictionary containing article data with at least 'title', 'description', and 'content'.
            
        Returns:
            str: The category name or 'irrelevante' if the article doesn't match any category.
        """
        # Combine all text fields for analysis
        text = ' '.join([
            article.get('title', ''),
            article.get('description', ''),
            article.get('content', '')
        ]).lower()
        
        # First check if the article is relevant
        if not self.is_relevant(text):
            return 'irrelevante'
            
        # Get title and description for special cases
        title = article.get('title', '').lower()
        description = article.get('description', '').lower()
        title_desc = f"{title} {description}"
        
        # Check for multi-word phrases that indicate specific categories
        # This helps with context that might be missed by single-word matching
        health_phrases = [
            'novo medicamento', 'nova medica√ß√£o', 'novo tratamento', 'nova terapia',
            'aprova√ß√£o de medicamento', 'aprova√ß√£o de tratamento', 'libera√ß√£o de medicamento',
            'estudo de medicamento', 'pesquisa de medicamento', 'ensaio cl√≠nico',
            'benef√≠cios do tratamento', 'efeitos do tratamento', 'efic√°cia do tratamento'
        ]
        
        family_phrases = [
            'desafios dos pais', 'desafios das m√£es', 'desafios das fam√≠lias',
            'dificuldades dos cuidadores', 'sobrecarga dos cuidadores', 'estresse dos pais',
            'experi√™ncia parental', 'experi√™ncia familiar', 'rotina familiar',
            'impacto na fam√≠lia', 'impacto nos pais', 'impacto no dia a dia'
        ]
        
        discrimination_phrases = [
            'tratamento diferenciado', 'olhares diferentes', 'coment√°rios inapropriados',
            'falta de compreens√£o', 'falta de empatia', 'falta de inclus√£o',
            'barreira atitudinal', 'barreira social', 'n√£o aceita√ß√£o',
            'exclus√£o social', 'isolamento social', 'segrega√ß√£o social'
        ]
        
        # Check for multi-word phrase matches first
        # Health treatment phrases
        health_phrase_score = sum(15 for phrase in health_phrases if phrase in text)
        if health_phrase_score >= 15:
            return 'saude_tratamento'
            
        # Family challenges phrases
        family_phrase_score = sum(15 for phrase in family_phrases if phrase in text)
        if family_phrase_score >= 15:
            return 'familia_cuidadores'
            
        # Discrimination phrases
        discrimination_phrase_score = sum(15 for phrase in discrimination_phrases if phrase in text)
        if discrimination_phrase_score >= 15:
            return 'violencia_discriminacao'
        
        # Special case 1: Check for violence/discrimination first (highest priority)
        violence_terms = self.categories['violencia_discriminacao']
        violence_score = sum(10 for term in violence_terms if term in title_desc)  # Higher weight for title/desc
        violence_score += sum(1 for term in violence_terms if term in text)  # Lower weight for full text
        
        if violence_score >= 3:  # Threshold for violence/discrimination
            return 'violencia_discriminacao'
            
        # Special case 2: Check for legislation/rights (high priority)
        rights_terms = self.categories['direitos_legislacao']
        rights_score = sum(5 for term in rights_terms if term in title_desc)
        rights_score += sum(1 for term in rights_terms if term in text)
        
        # Special case 3: Check for research/statistics (medium priority)
        research_terms = self.categories['pesquisa_estatistica']
        research_score = sum(3 for term in research_terms if term in title_desc)
        research_score += sum(1 for term in research_terms if term in text)
        
        # Only classify as research if it's specifically about autism research
        is_about_autism = any(term in text for term in ['autis', 'TEA', 'transtorno do espectro autista'])
        
        # If it's about rights/legislation and not just a general research article
        if rights_score >= 5 and 'direito' in text:
            return 'direitos_legislacao'
            
        # If it's specifically about autism research
        if research_score >= 3 and is_about_autism and 'pesquisa' in text:
            return 'pesquisa_estatistica'
            
        # Special case for health/treatment (medication, therapy, etc.)
        health_terms = self.categories['saude_tratamento']
        health_score = sum(5 for term in health_terms if term in title_desc)
        health_score += sum(1 for term in health_terms if term in text)
        
        if health_score >= 5 and any(term in text for term in ['medicamento', 'medica√ß√£o', 'rem√©dio', 'terapia', 'tratamento']):
            return 'saude_tratamento'
            
        # Special case for family/caregivers
        family_terms = self.categories['familia_cuidadores']
        family_score = sum(5 for term in family_terms if term in title_desc)
        family_score += sum(1 for term in family_terms if term in text)
        
        if family_score >= 5 and any(term in text for term in ['fam√≠lia', 'pais', 'm√£es', 'cuidadores', 'desafio']):
            return 'familia_cuidadores'
        
        # Calculate scores for all categories with weights
        category_scores = {}
        
        for category, keywords in self.categories.items():
            # Skip categories we already checked
            if category in ['violencia_discriminacao', 'direitos_legislacao', 'pesquisa_estatistica']:
                continue
                
            # Initialize score for this category
            score = 0
            
            # Higher weight for title matches (3x)
            if 'title' in article:
                title = article['title'].lower()
                score += sum(3 for keyword in keywords if keyword in title)
                
            # Medium weight for description matches (2x)
            if 'description' in article:
                desc = article['description'].lower()
                score += sum(2 for keyword in keywords if keyword in desc)
                
            # Lower weight for content matches (1x)
            if 'content' in article:
                content = article['content'].lower()
                score += sum(1 for keyword in keywords if keyword in content)
            
            # Only add to scores if we found matches
            if score > 0:
                category_scores[category] = score
        
        # If we have category matches, return the highest scoring one
        if category_scores:
            best_category = max(category_scores.items(), key=lambda x: x[1])[0]
            # Only return if score is above threshold
            if category_scores[best_category] >= 3:  # Minimum threshold
                return best_category
        
        # Special case: Check for autism-related research that might have been missed
        research_terms = ['pesquisa', 'estudo', 'levantamento', 'dados', 'estat√≠stica', 'censo']
        if any(term in text for term in research_terms) and \
           any(term in text for term in ['autis', 'TEA', 'transtorno do espectro autista']):
            return 'pesquisa_estatistica'
            
        # Special case: Check for rights/legislation that might have been missed
        rights_terms = ['direito', 'lei', 'legisla√ß√£o', 'projeto de lei', 'PL', 'proposta']
        if any(term in text for term in rights_terms) and \
           any(term in text for term in ['autis', 'TEA', 'transtorno do espectro autista']):
            return 'direitos_legislacao'
        
        # If we get here, no category matched well enough
        return 'outros'

    async def cluster_recent_news(self, country: str = 'BR', force_update: bool = False) -> Dict[str, Any]:
        """Cluster recent news articles into topics and extract facts.
        
        Args:
            country: Country code to filter news by
            force_update: Whether to force reclustering even if recent clustering was performed
            
        Returns:
            Dictionary with clustering results and statistics
        """
        try:
            start_time = datetime.utcnow()
            logger.info(f"üöÄ Iniciando clustering de not√≠cias para {country}")
            
            # Conectar ao banco de dados
            db_manager = MongoDBManager()
            await db_manager.connect_to_mongodb()
            
            async with db_manager.get_db() as db:
                # Check if recent clustering was performed (unless forced)
                if not force_update:
                    recent_clustering = await db.clustering_logs.find_one(
                        {
                            'country': country,
                            'status': 'completed',
                            'completed_at': {'$gte': datetime.utcnow() - timedelta(hours=6)}
                        },
                        sort=[('completed_at', -1)]
                    )
                    
                    if recent_clustering:
                        logger.info(f"‚è≠Ô∏è Clustering recente encontrado para {country}, pulando...")
                        return {
                            'status': 'skipped',
                            'reason': 'recent_clustering_exists',
                            'last_clustering': recent_clustering['completed_at']
                        }
                
                # Log do in√≠cio do clustering
                log_id = await self._log_clustering_start(db, country)
                
                try:
                    # 1. Buscar artigos n√£o categorizados
                    uncategorized_articles = await self._get_uncategorized_articles(db, country)
                    logger.info(f"üìä Encontrados {len(uncategorized_articles)} artigos n√£o categorizados")
                    
                    if not uncategorized_articles:
                        logger.info("‚ÑπÔ∏è Nenhum artigo novo para clustering")
                        await self._log_clustering_completion(db, log_id, 0, 0)
                        return {
                            'status': 'completed',
                            'topics_created': 0,
                            'articles_processed': 0,
                            'message': 'No new articles to cluster'
                        }
                    
                    # 2. Categorizar artigos
                    categorized_articles = {}
                    for article in uncategorized_articles:
                        category = self._categorize_article(article)
                        if category not in categorized_articles:
                            categorized_articles[category] = []
                        categorized_articles[category].append(article)
                    
                    logger.info(f"üìÅ Artigos categorizados em {len(categorized_articles)} categorias")
                    
                    # 3. Fazer clustering por categoria
                    total_topics_created = 0
                    total_articles_processed = 0
                    
                    for category, articles in categorized_articles.items():
                        if category == 'irrelevante':
                            # Marcar como processados mas n√£o criar t√≥picos
                            await self._mark_articles_as_processed(db, articles, 'irrelevante')
                            total_articles_processed += len(articles)
                            continue
                        
                        logger.info(f"üîç Clustering categoria '{category}' ({len(articles)} artigos)")
                        
                        # Clustering dentro da categoria
                        topics_created = await self._cluster_by_category(db, articles, category, country)
                        total_topics_created += topics_created
                        total_articles_processed += len(articles)
                        
                        logger.info(f"‚úÖ Categoria '{category}': {topics_created} t√≥picos criados")
                    
                    # 4. NOVA SE√á√ÉO: P√≥s-processamento de fatos para todos os t√≥picos
                    await self._postprocess_all_topics_facts(db, country)
                    
                    # 5. Log de conclus√£o
                    await self._log_clustering_completion(db, log_id, total_topics_created, total_articles_processed)
                    
                    end_time = datetime.utcnow()
                    duration = (end_time - start_time).total_seconds()
                    
                    logger.info(f"üéâ Clustering conclu√≠do: {total_topics_created} t√≥picos, {total_articles_processed} artigos em {duration:.2f}s")
                    
                    return {
                        'status': 'completed',
                        'topics_created': total_topics_created,
                        'articles_processed': total_articles_processed,
                        'categories_processed': list(categorized_articles.keys()),
                        'duration_seconds': duration,
                        'facts_extracted': True  # Novo campo
                    }
                    
                except Exception as e:
                    # Log de erro
                    await self._log_clustering_error(db, log_id, str(e))
                    raise
                    
        except Exception as e:
            logger.error(f"‚ùå Erro no clustering: {str(e)}", exc_info=True)
            raise


    async def _cluster_by_category(self, db, articles: List[Dict], category: str, country: str) -> int:
        """Cluster articles within a specific category and extract facts.
        
        Args:
            db: Database connection
            articles: List of articles to cluster
            category: Category name
            country: Country code
            
        Returns:
            Number of topics created
        """
        if len(articles) < 2:
            # Para artigos √∫nicos, criar t√≥pico individual
            if articles:
                topic_id = await self._create_single_article_topic(db, articles[0], category, country)
                # INTEGRA√á√ÉO: Extrair fatos do t√≥pico √∫nico
                if topic_id:
                    await self._preprocess_topic_facts(db, str(topic_id), f"T√≥pico individual - {category}")
                return 1
            return 0
        
        try:
            # Extrair embeddings
            embeddings = []
            valid_articles = []
            
            for article in articles:
                embedding = article.get('embedding')
                if embedding and len(embedding) > 0:
                    embeddings.append(embedding)
                    valid_articles.append(article)
            
            if len(valid_articles) < 2:
                logger.warning(f"N√£o h√° embeddings suficientes para clustering na categoria {category}")
                return 0
            
            # Aplicar DBSCAN clustering
            embeddings_array = np.array(embeddings)
            
            # Par√¢metros ajustados para melhor clustering
            eps = 0.3 if len(valid_articles) > 10 else 0.4
            min_samples = max(2, min(3, len(valid_articles) // 4))
            
            clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
            cluster_labels = clustering.fit_predict(embeddings_array)
            
            # Agrupar artigos por cluster
            clusters = {}
            noise_articles = []
            
            for idx, label in enumerate(cluster_labels):
                if label == -1:  # Ru√≠do
                    noise_articles.append(valid_articles[idx])
                else:
                    if label not in clusters:
                        clusters[label] = []
                    clusters[label].append(valid_articles[idx])
            
            # Criar t√≥picos para cada cluster
            topics_created = 0
            
            for cluster_id, cluster_articles in clusters.items():
                if len(cluster_articles) >= 2:  # M√≠nimo 2 artigos por t√≥pico
                    topic_id = await self._create_topic_from_cluster(db, cluster_articles, category, country)
                    if topic_id:
                        # INTEGRA√á√ÉO: Extrair fatos do t√≥pico
                        topic_title = await self._get_topic_title(db, topic_id)
                        await self._preprocess_topic_facts(db, str(topic_id), topic_title)
                        topics_created += 1
                else:
                    noise_articles.extend(cluster_articles)
            
            # Processar artigos de ru√≠do como t√≥picos individuais
            for article in noise_articles:
                topic_id = await self._create_single_article_topic(db, article, category, country)
                if topic_id:
                    # INTEGRA√á√ÉO: Extrair fatos do t√≥pico individual
                    await self._preprocess_topic_facts(db, str(topic_id), f"T√≥pico individual - {category}")
                    topics_created += 1
            
            return topics_created
            
        except Exception as e:
            logger.error(f"Erro no clustering da categoria {category}: {str(e)}", exc_info=True)
            return 0


    async def _postprocess_all_topics_facts(self, db, country: str) -> None:
        """P√≥s-processa fatos para todos os t√≥picos que ainda n√£o foram processados.
        
        Args:
            db: Database connection
            country: Country code
        """
        try:
            logger.info(f"üß† Iniciando p√≥s-processamento de fatos para t√≥picos do pa√≠s {country}")
            
            # Buscar t√≥picos que n√£o tiveram fatos processados
            unprocessed_topics = await db.topics.find({
                'country_focus': country,
                'is_active': True,
                '$or': [
                    {'facts_processed': {'$exists': False}},
                    {'facts_processed': False},
                    {'facts_processed_at': {'$lt': datetime.utcnow() - timedelta(days=1)}}  # Reprocessar diariamente
                ]
            }).to_list(length=None)
            
            logger.info(f"üîç Encontrados {len(unprocessed_topics)} t√≥picos para processamento de fatos")
            
            facts_processed_count = 0
            facts_failed_count = 0
            
            for topic in unprocessed_topics:
                topic_id = str(topic['_id'])
                topic_name = topic.get('title', 'T√≥pico sem nome')
                
                try:
                    await self._preprocess_topic_facts(db, topic_id, topic_name)
                    facts_processed_count += 1
                except Exception as e:
                    logger.error(f"‚ùå Erro ao processar fatos do t√≥pico {topic_name}: {str(e)}")
                    facts_failed_count += 1
            
            logger.info(f"‚úÖ P√≥s-processamento conclu√≠do: {facts_processed_count} sucessos, {facts_failed_count} falhas")
            
        except Exception as e:
            logger.error(f"‚ùå Erro no p√≥s-processamento de fatos: {str(e)}", exc_info=True)


    async def _get_topic_title(self, db, topic_id: ObjectId) -> str:
        """Busca o t√≠tulo de um t√≥pico pelo ID.
        
        Args:
            db: Database connection
            topic_id: ObjectId do t√≥pico
            
        Returns:
            T√≠tulo do t√≥pico ou string padr√£o
        """
        try:
            topic = await db.topics.find_one({'_id': topic_id}, {'title': 1})
            return topic.get('title', 'T√≥pico sem nome') if topic else 'T√≥pico n√£o encontrado'
        except Exception:
            return 'T√≥pico desconhecido'


    async def _create_topic_from_cluster(self, db, articles: List[Dict], category: str, country: str) -> Optional[ObjectId]:
        """Cria um t√≥pico a partir de um cluster de artigos.
        
        Args:
            db: Database connection
            articles: Lista de artigos do cluster
            category: Categoria do t√≥pico
            country: C√≥digo do pa√≠s
            
        Returns:
            ObjectId do t√≥pico criado ou None se falhou
        """
        try:
            # Gerar t√≠tulo e descri√ß√£o usando IA
            title, description = await self._generate_topic_metadata(articles)
            
            # Calcular embedding m√©dio
            embeddings = [article['embedding'] for article in articles if article.get('embedding')]
            avg_embedding = np.mean(embeddings, axis=0).tolist() if embeddings else []
            
            # Preparar dados do t√≥pico
            topic_data = {
                'title': title,
                'description': description,
                'category': category,
                'country_focus': country,
                'articles': [str(article['_id']) for article in articles],
                'article_count': len(articles),
                'sources': list(set(article.get('domain', '') for article in articles if article.get('domain'))),
                'created_at': datetime.utcnow(),
                'updated_at': datetime.utcnow(),
                'is_active': True,
                'embedding': avg_embedding,
                # Campos para fatos (ser√£o preenchidos depois)
                'facts_processed': False,
                'extracted_facts': [],
                'facts_summary': {},
                'total_facts_available': 0
            }
            
            # Inserir no banco
            result = await db.topics.insert_one(topic_data)
            
            # Marcar artigos como processados
            article_ids = [article['_id'] for article in articles]
            await db.news.update_many(
                {'_id': {'$in': article_ids}},
                {
                    '$set': {
                        'clustered': True,
                        'topic_id': str(result.inserted_id),
                        'updated_at': datetime.utcnow()
                    }
                }
            )
            
            logger.debug(f"‚úÖ T√≥pico criado: {title} (ID: {result.inserted_id})")
            return result.inserted_id
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar t√≥pico: {str(e)}", exc_info=True)
            return None


    # O m√©todo _preprocess_topic_facts j√° existe e est√° correto!
    # Basta garantir que ele seja chamado nos momentos certos acima.

    # Tamb√©m adicione este m√©todo de utilidade:

    async def _mark_articles_as_processed(self, db, articles: List[Dict], reason: str) -> None:
        """Marca artigos como processados sem criar t√≥picos.
        
        Args:
            db: Database connection
            articles: Lista de artigos para marcar
            reason: Motivo do processamento (ex: 'irrelevante')
        """
        try:
            article_ids = [article['_id'] for article in articles]
            await db.news.update_many(
                {'_id': {'$in': article_ids}},
                {
                    '$set': {
                        'clustered': True,
                        'cluster_reason': reason,
                        'updated_at': datetime.utcnow()
                    }
                }
            )
            logger.debug(f"‚úÖ {len(articles)} artigos marcados como processados ({reason})")
        except Exception as e:
            logger.error(f"‚ùå Erro ao marcar artigos como processados: {str(e)}", exc_info=True)

    async def _log_clustering_start(self, db, country: str) -> str:
        """Log in√≠cio do clustering."""
        try:
            log_doc = {
                'country': country,
                'status': 'started',
                'started_at': datetime.utcnow(),
                'process_id': f"clustering_{country}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
            }
            result = await db.clustering_logs.insert_one(log_doc)
            logger.info(f"üìù Log de clustering iniciado: {result.inserted_id}")
            return str(result.inserted_id)
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar log de clustering: {str(e)}")
            return None

    async def _log_clustering_completion(self, db, log_id: str, topics_created: int, articles_processed: int) -> None:
        """Log conclus√£o do clustering."""
        try:
            if log_id and ObjectId.is_valid(log_id):
                await db.clustering_logs.update_one(
                    {'_id': ObjectId(log_id)},
                    {
                        '$set': {
                            'status': 'completed',
                            'completed_at': datetime.utcnow(),
                            'topics_created': topics_created,
                            'articles_processed': articles_processed,
                            'duration_seconds': None  # Ser√° calculado se necess√°rio
                        }
                    }
                )
                logger.info(f"üìù Log de clustering atualizado: {log_id}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao atualizar log de clustering: {str(e)}")

    async def _log_clustering_error(self, db, log_id: str, error: str) -> None:
        """Log erro do clustering."""
        try:
            if log_id and ObjectId.is_valid(log_id):
                await db.clustering_logs.update_one(
                    {'_id': ObjectId(log_id)},
                    {
                        '$set': {
                            'status': 'error',
                            'error_at': datetime.utcnow(),
                            'error_message': error[:1000],  # Limitar tamanho do erro
                            'completed_at': datetime.utcnow()
                        }
                    }
                )
                logger.error(f"üìù Log de erro registrado: {log_id}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao registrar erro no log: {str(e)}")

    async def _get_uncategorized_articles(self, db, country: str) -> List[Dict]:
        """Busca artigos que ainda n√£o foram categorizados."""
        try:
            # Buscar artigos n√£o clusterizados do pa√≠s
            articles = await db.news.find({
                'country_focus': country.upper(),
                '$or': [
                    {'clustered': {'$exists': False}},
                    {'clustered': False}
                ]
            }).to_list(length=None)
            
            logger.info(f"üîç Encontrados {len(articles)} artigos n√£o categorizados para {country}")
            return articles
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar artigos n√£o categorizados: {str(e)}")
            return []

    async def _create_single_article_topic(self, db, article: Dict, category: str, country: str) -> Optional[ObjectId]:
        """Cria um t√≥pico para um artigo √∫nico."""
        try:
            title = article.get('title', 'T√≥pico sem t√≠tulo')
            description = f"T√≥pico criado a partir do artigo: {title[:100]}..."
            
            topic_data = {
                'title': title,
                'description': description,
                'category': category,
                'country_focus': country,
                'articles': [str(article['_id'])],
                'article_count': 1,
                'sources': [article.get('source_domain', '')],
                'created_at': datetime.utcnow(),
                'updated_at': datetime.utcnow(),
                'is_active': True,
                'embedding': article.get('embedding', []),
                # Campos para fatos
                'facts_processed': False,
                'extracted_facts': [],
                'facts_summary': {},
                'total_facts_available': 0
            }
            
            result = await db.topics.insert_one(topic_data)
            
            # Marcar artigo como processado
            await db.news.update_one(
                {'_id': article['_id']},
                {
                    '$set': {
                        'clustered': True,
                        'topic_id': str(result.inserted_id),
                        'updated_at': datetime.utcnow()
                    }
                }
            )
            
            logger.debug(f"‚úÖ T√≥pico individual criado: {title[:50]}...")
            return result.inserted_id
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar t√≥pico individual: {str(e)}")
            return None

    async def _generate_topic_metadata(self, articles: List[Dict]) -> Tuple[str, str]:
        """Gera t√≠tulo e descri√ß√£o para um t√≥pico baseado nos artigos."""
        try:
            # Extrair t√≠tulos dos artigos
            titles = [article.get('title', '') for article in articles if article.get('title')]
            
            if not titles:
                return "T√≥pico sem t√≠tulo", "T√≥pico agrupado automaticamente"
            
            # Usar o t√≠tulo mais comum ou o primeiro se n√£o houver padr√£o
            if len(titles) == 1:
                title = titles[0]
            else:
                # Encontrar palavras comuns nos t√≠tulos
                words = []
                for title in titles:
                    words.extend(title.lower().split())
                
                common_words = [word for word, count in Counter(words).most_common(3) 
                            if len(word) > 3 and word not in ['para', 'sobre', 'como', 'mais']]
                
                if common_words:
                    title = f"Not√≠cias sobre {' '.join(common_words[:2])}"
                else:
                    title = titles[0]  # Fallback para o primeiro t√≠tulo
            
            # Limitar tamanho do t√≠tulo
            title = title[:100] if len(title) > 100 else title
            
            # Gerar descri√ß√£o
            description = f"T√≥pico agrupando {len(articles)} artigos relacionados"
            
            return title, description
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar metadados do t√≥pico: {str(e)}")
            return "T√≥pico agrupado", f"T√≥pico com {len(articles)} artigos"
# Create a singleton instance
topic_cluster = TopicCluster()
