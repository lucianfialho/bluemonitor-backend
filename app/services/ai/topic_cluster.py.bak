"""Topic clustering service for grouping related news articles."""
import logging
import re
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from datetime import datetime, timedelta
from bson import ObjectId
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity

from app.core.config import settings
from app.core.database import MongoDBManager
from app.services.ai.processor import ai_processor

logger = logging.getLogger(__name__)

class TopicCluster:
    """Service for clustering news articles into topics."""
    
    def __init__(self):
        """Initialize the topic clustering service."""
        self.min_samples = 1  # Permite clusters menores
        self.eps = 0.85  # Ajustado para melhor equilíbrio
        self.min_topic_size = 1  # Permite tópicos com apenas 1 artigo
        self.max_topic_age_days = 30  # Período maior para análise
        self.max_articles_to_process = 1000  # Aumentado para incluir mais artigos
        self.similarity_threshold = 0.4  # Reduzido para agrupar tópicos mais diversos
        
        # Categorias pré-definidas para classificação inicial
        self.categories = {
            'Saúde': ['saúde', 'médico', 'tratamento', 'terapia', 'autismo', 'TEA', 'diagnóstico', 'intervenção', 'saudável'],
            'Direitos': ['direito', 'lei', 'legislação', 'jurídico', 'justiça', 'direitos humanos', 'inclusão', 'acessibilidade'],
            'Tecnologia': ['tecnologia', 'app', 'aplicativo', 'software', 'hardware', 'inovação', 'digital', 'tecnológico'],
            'Educação': ['educação', 'escola', 'ensino', 'aprendizado', 'professor', 'aluno', 'pedagogia', 'inclusão escolar'],
            'Pesquisa': ['pesquisa', 'estudo', 'científico', 'ciência', 'descoberta', 'universidade', 'pesquisador'],
            'Família': ['família', 'pais', 'mãe', 'pai', 'filho', 'cuidadores', 'casa', 'lar'],
            'Inclusão': ['inclusão', 'acessibilidade', 'inclusivo', 'diversidade', 'equidade', 'oportunidades']
        }
        
        # Palavras-chave para identificar notícias irrelevantes
        self.irrelevant_keywords = [
            'futebol', 'esporte', 'celebridade', 'entretenimento', 'novela', 'cinema',
            'música', 'show', 'festival', 'bbb', 'big brother', 'lazer', 'viagem', 'turismo'
        ]
    
    def _categorize_article(self, article: Dict[str, Any]) -> str:
        """Categorize an article into one of the predefined categories.
        
        Args:
            article: The article to categorize.
            
        Returns:
            str: The category name or 'Outros' if no match is found.
        """
        # Combine title, description, and content for analysis
        text = ' '.join([
            article.get('title', ''),
            article.get('description', ''),
            article.get('content', '')
        ]).lower()
        
        # Check for irrelevant content first
        if any(keyword in text for keyword in self.irrelevant_keywords):
            return 'Irrelevante'
        
        # Calculate category scores
        category_scores = {}
        for category, keywords in self.categories.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                category_scores[category] = score
        
        # Return the category with the highest score, or 'Outros' if no match
        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        return 'Outros'
    
    async def _cluster_by_category(self, articles: List[Dict[str, Any]], category: str) -> None:
        """Cluster articles within a specific category.
        
        Args:
            articles: List of articles in the category.
            category: The category name.
        """
        if not articles:
            return
            
        logger.info(f"Clustering {len(articles)} articles in category: {category}")
        
        # Get embeddings for clustering
        article_embeddings = []
        valid_articles = []
        
        for article in articles:
            if 'embedding' in article and article['embedding']:
                article_embeddings.append(article['embedding'])
                valid_articles.append(article)
        
        if not valid_articles:
            logger.warning(f"No valid embeddings found for category: {category}")
            return
            
        # Convert to numpy array
        X = np.array(article_embeddings)
        
        # Normalize the vectors for cosine similarity
        from sklearn.preprocessing import normalize
        X_normalized = normalize(X)
        
        # Adjust parameters based on category
        eps = self.eps
        min_samples = self.min_samples
        
        # Categories that might need more specific clustering
        if category in ['Saúde', 'Pesquisa']:
            eps = 0.8  # Tighter clustering for more specific topics
        elif category in ['Família', 'Inclusão']:
            eps = 0.9  # Broader clustering for more general topics
            
        logger.info(f"Clustering with eps={eps}, min_samples={min_samples} for category: {category}")
        
        # Apply DBSCAN
        clustering = DBSCAN(
            eps=eps,
            min_samples=min_samples,
            metric='cosine',
            n_jobs=-1
        ).fit(X_normalized)
        
        # Process clusters
        labels = clustering.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        logger.info(f"Found {n_clusters} clusters in category: {category}")
        
        # Group articles by cluster
        clusters: Dict[int, List[Dict[str, Any]]] = {}
        for i, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(valid_articles[i])
        
        # Process each cluster
        for label, cluster_articles in clusters.items():
            if label == -1 or len(cluster_articles) < self.min_topic_size:
                # Skip noise and small clusters
                continue
                
            # Process the cluster into a topic
            await self._process_topic(cluster_articles, cluster_articles[0].get('country_focus', 'BR'))
    
    async def cluster_recent_news(self, country: str = 'BR') -> None:
        """Cluster recent news articles into topics with category-based clustering.
        
        Args:
            country: Country code to filter news (default: 'BR' for Brazil).
        """
        logger.info(f"Starting topic clustering for {country}...")
        
        # Create a new MongoDBManager instance
        mongodb_manager = MongoDBManager()
        await mongodb_manager.connect_to_mongodb()
        
        try:
            # Get recent news that haven't been clustered yet
            async with mongodb_manager.get_db() as db:
                # Primeiro, verificar quantos artigos existem no total
                total_articles = await db.news.count_documents({})
                logger.info(f"Total de artigos no banco: {total_articles}")
                
                # Verificar quantos artigos têm embeddings
                with_embeddings = await db.news.count_documents({"embedding": {"$exists": True, "$ne": None}})
                logger.info(f"Artigos com embeddings: {with_embeddings}")
                
                # Verificar quantos artigos já estão em tópicos
                in_topic = await db.news.count_documents({"in_topic": True})
                logger.info(f"Artigos já em tópicos: {in_topic}")
                
                # Consulta para buscar artigos não processados
                query = {
                    "country_focus": country.upper(),
                    "embedding": {"$exists": True, "$ne": None},
                    "in_topic": {"$ne": True}
                }
                
                logger.info(f"Querying news with filter: {query}")
                count = await db.news.count_documents(query)
                logger.info(f"Found {count} articles matching the query")
                
                # Se não encontrou, tentar sem o filtro de país
                if count == 0:
                    query = {"embedding": {"$exists": True, "$ne": None}}
                    count = await db.news.count_documents(query)
                    logger.info(f"Found {count} articles with embeddings (without country filter)")
                
                # Buscar os artigos mais recentes
                recent_news = await db.news.find(query) \
                    .sort("publish_date", -1) \
                    .limit(self.max_articles_to_process) \
                    .to_list(length=None)
                    
                logger.info(f"Retrieved {len(recent_news)} articles for processing")
                
                if not recent_news:
                    logger.info("No new articles to cluster")
                    return
                
                # Categorizar os artigos
                categorized_articles = {category: [] for category in self.categories.keys()}
                categorized_articles['Outros'] = []
                categorized_articles['Irrelevante'] = []
                
                for article in recent_news:
                    category = self._categorize_article(article)
                    categorized_articles[category].append(article)
                
                # Log category distribution
                for category, articles in categorized_articles.items():
                    if articles:
                        logger.info(f"Category '{category}': {len(articles)} articles")
                
                # Processar cada categoria separadamente
                for category, articles in categorized_articles.items():
                    if articles and category != 'Irrelevante':  # Ignorar irrelevantes
                        await self._cluster_by_category(articles, category)
                
                # Marcar artigos irrelevantes como processados para não aparecerem novamente
                if categorized_articles['Irrelevante']:
                    irrelevant_ids = [str(a['_id']) for a in categorized_articles['Irrelevante']]
                    await db.news.update_many(
                        {"_id": {"$in": irrelevant_ids}},
                        {"$set": {"in_topic": True, "is_irrelevant": True}}
                    )
                    logger.info(f"Marked {len(irrelevant_ids)} irrelevant articles as processed")
            
            logger.info(f"Completed topic clustering for {len(recent_news)} articles")
                
        except Exception as e:
            logger.error(f"Error in topic clustering: {str(e)}", exc_info=True)
            raise
            
        finally:
            # Ensure the connection is closed
            if 'mongodb_manager' in locals():
                await mongodb_manager.close_mongodb_connection()

    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        """Parse a date string in various formats to a timezone-naive datetime.
        
        Args:
            date_str: The date string to parse.
            
        Returns:
            A timezone-naive datetime or None if parsing fails.
        """
        from datetime import datetime, timezone
        import re
        
        if not date_str or not isinstance(date_str, str):
            return None
            
        # Remove any leading/trailing whitespace
        date_str = date_str.strip()
        
        # Common date formats to try
        formats = [
            '%Y-%m-%dT%H:%M:%S%z',  # ISO format with timezone
            '%Y-%m-%dT%H:%M:%S',     # ISO format without timezone
            '%Y-%m-%d %H:%M:%S',     # SQL format
            '%d/%m/%Y %H:%M',        # Common Brazilian format with time
            '%d/%m/%Y | %Hh%M',      # Format seen in the logs
            '%d/%m/%Y',              # Just date
            '%Y-%m-%d',              # ISO date
        ]
        
        # Try each format
        for fmt in formats:
            try:
                dt = datetime.strptime(date_str, fmt)
                # If timezone-aware, convert to UTC and make naive
                if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
                    dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
                return dt
            except (ValueError, AttributeError):
                continue
                
        # Try to extract date from complex strings
        try:
            # Look for common date patterns
            date_match = re.search(r'(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})', date_str)
            if date_match:
                date_part = date_match.group(1)
                return self._parse_date_string(date_part)
        except Exception:
            pass
            
        return None
    
    def _get_article_date(self, article: Dict[str, Any]) -> datetime:
        """Get the publish date from an article, with fallback to current date if not available.
        
        Args:
            article: The article dictionary.
            
        Returns:
            The article's publish date or current date if not available, as timezone-naive UTC datetime.
        """
        from datetime import datetime, timezone
        
        publish_date = article.get('publish_date')
        
        # If no date or empty, use current time
        if not publish_date:
            return datetime.utcnow().replace(tzinfo=None)
        
        # If already a datetime object
        if isinstance(publish_date, datetime):
            dt = publish_date
            # If timezone-aware, convert to UTC and make naive
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        
        # If string, try to parse it
        if isinstance(publish_date, str):
            parsed_date = self._parse_date_string(publish_date)
            if parsed_date is not None:
                return parsed_date
        
        # If we get here, we couldn't parse the date
        logger.warning(f"Could not parse date: {publish_date}")
        return datetime.utcnow().replace(tzinfo=None)
    
    async def _process_topic(self, articles: List[Dict[str, Any]], country: str) -> None:
        """Process a cluster of articles into a topic.
        
        Args:
            articles: List of articles in the cluster.
            country: Country code.
        """
        if not articles:
            logger.warning("No articles provided to process")
            return
            
        logger.info(f"Processing {len(articles)} articles for topic creation")
        
        try:
            # Determinar a categoria principal do cluster
            category_scores = {}
            for article in articles:
                category = self._categorize_article(article)
                if category != 'Irrelevante':  # Ignorar categoria irrelevante
                    category_scores[category] = category_scores.get(category, 0) + 1
            
            main_category = max(category_scores.items(), key=lambda x: x[1])[0] if category_scores else 'Geral'
            
            # Process each article individually
            for article in articles:
                try:
                    # Skip if not a dictionary or already in a topic
                    if not isinstance(article, dict) or article.get('in_topic'):
                        continue
                        
                    # Try to find a similar existing topic in the same category
                    similar_topic = await self._find_similar_topic(article, country)
                    
                    if similar_topic:
                        # Ensure we have a valid article with _id
                        if not article.get('_id'):
                            logger.warning(f"Article missing _id, skipping: {article.get('title', 'No title')}")
                            continue
                            
                        # Add to existing topic
                        await self._update_existing_topic(similar_topic, [article])
                        logger.info(f"Added article to existing topic: {article.get('title', 'No title')}")
                    else:
                        # Ensure we have a valid article with _id
                        if not article.get('_id'):
                            logger.warning(f"Article missing _id, cannot create topic: {article.get('title', 'No title')}")
                            continue
                            
                        # Create new topic with this article as the main one and the determined category
                        await self._create_new_topic(article, [article], country, main_category)
                        logger.info(f"Created new topic in category '{main_category}' for article: {article.get('title', 'No title')}")
                            
                except Exception as e:
                    logger.error(f"Error processing article {article.get('_id')}: {str(e)}", exc_info=True)
            
            # Tentar mesclar tópicos similares na mesma categoria
            logger.info(f"Tentando mesclar tópicos similares na categoria '{main_category}'...")
            await self._merge_similar_topics(country, main_category)
            
        except Exception as e:
            logger.error(f"Error in topic processing: {str(e)}", exc_info=True)
            raise
    
    async def _merge_similar_topics(self, country: str, category: Optional[str] = None) -> None:
        """Tenta mesclar tópicos similares, considerando a categoria se fornecida.
        
        Args:
            country: Código do país para filtrar tópicos.
            category: Categoria opcional para restringir a mesclagem a tópicos da mesma categoria.
        """
        # Create a new MongoDBManager instance
        mongodb_manager = MongoDBManager()
        await mongodb_manager.connect_to_mongodb()
        
        try:
            async with mongodb_manager.get_db() as db:
                # Build query to find active topics
                query = {
                    "country_focus": country.upper(),
                    "is_active": True
                }
                
                # Add category filter if provided
                if category:
                    query["category"] = category
                
                # Busca todos os tópicos ativos (com filtro de categoria, se aplicável)
                topics = await db.topics.find(query).to_list(length=200)  # Aumentado o limite para 200
                
                if len(topics) <= 1:
                    logger.info(f"Nenhum tópico para mesclar{ ' na categoria ' + category if category else '' }")
                    return
                
                logger.info(f"Verificando {len(topics)} tópicos ativos para mesclagem{ ' na categoria ' + category if category else '' }")
                
                # Para cada par de tópicos, verifica se são similares
                merged = set()
                for i in range(len(topics)):
                    if topics[i]['_id'] in merged:
                        continue
                        
                    for j in range(i + 1, len(topics)):
                        if topics[j]['_id'] in merged:
                            continue
                            
                        topic1 = topics[i]
                        topic2 = topics[j]
                        
                        # Skip if topics are from different categories (unless category is None)
                        if category is None and topic1.get('category') != topic2.get('category'):
                            continue
                        
                        # Calculate similarity between topic embeddings
                        if 'embedding' not in topic1 or 'embedding' not in topic2:
                            continue
                            
                        similarity = cosine_similarity(
                            np.array(topic1['embedding']).reshape(1, -1),
                            np.array(topic2['embedding']).reshape(1, -1)
                        )[0][0]
                        
                        # Se a similaridade for maior que o limiar, mescla os tópicos
                        if similarity > self.similarity_threshold:
                            try:
                                logger.info(f"Mesclando tópicos '{topic1.get('title', '')[:50]}' e '{topic2.get('title', '')[:50]}' com similaridade {similarity:.2f}")
                                
                                # Decide qual tópico manter com base no número de artigos e na data de criação
                                if len(topic2.get('articles', [])) > len(topic1.get('articles', [])) or \
                                   (len(topic2.get('articles', [])) == len(topic1.get('articles', [])) and 
                                    topic2.get('created_at', datetime.min) > topic1.get('created_at', datetime.min)):
                                    # Swap para manter o tópico com mais artigos ou mais recente
                                    topic1, topic2 = topic2, topic1
                                
                                # Combina as palavras-chave dos dois tópicos
                                keywords = list(set(topic1.get('keywords', []) + topic2.get('keywords', [])))
                                
                                # Atualiza o tópico1 com os artigos do tópico2 e metadados combinados
                                await db.topics.update_one(
                                    {"_id": topic1['_id']},
                                    {
                                        "$addToSet": {"articles": {"$each": topic2['articles']}},
                                        "$set": {
                                            "updated_at": datetime.utcnow(),
                                            "article_count": len(topic1['articles']) + len(topic2['articles']),
                                            "keywords": keywords[:30],  # Limita a 30 palavras-chave
                                            "last_updated": datetime.utcnow()
                                        }
                                    }
                                )
                                
                                # Atualiza o título e resumo se o tópico2 tiver mais artigos
                                if len(topic2['articles']) > len(topic1['articles']):
                                    await db.topics.update_one(
                                        {"_id": topic1['_id']},
                                        {"$set": {
                                            "title": topic2.get('title'),
                                            "summary": topic2.get('summary'),
                                            "main_article_id": topic2.get('main_article_id')
                                        }}
                                    )
                                
                                # Marca o tópico2 como inativo
                                await db.topics.update_one(
                                    {"_id": topic2['_id']},
                                    {"$set": {
                                        "is_active": False,
                                        "merged_into": topic1['_id'],
                                        "merged_at": datetime.utcnow()
                                    }}
                                )
                                
                                # Atualiza os artigos para apontar para o tópico1
                                await db.news.update_many(
                                    {"topic_id": topic2['_id']},
                                    {"$set": {
                                        "topic_id": topic1['_id'],
                                        "topic_updated_at": datetime.utcnow()
                                    }}
                                )
                                
                                # Adiciona à lista de mesclados para evitar processamento duplicado
                                merged.add(topic2['_id'])
                                
                                # Atualiza a referência ao tópico1 na lista para refletir as mudanças
                                if topic1['_id'] == topics[i]['_id']:
                                    topics[i] = await db.topics.find_one({"_id": topic1['_id']})
                                
                            except Exception as merge_error:
                                logger.error(f"Erro ao mesclar tópicos {topic1.get('_id')} e {topic2.get('_id')}: {str(merge_error)}", 
                                           exc_info=True)
                                continue
                
                logger.info(f"Concluída a mesclagem de tópicos{ ' na categoria ' + category if category else '' }. {len(merged)} tópicos mesclados.")
                
        except Exception as e:
            logger.error(f"Erro ao mesclar tópicos: {str(e)}", exc_info=True)
            raise
    
    async def _find_similar_topic(
        self, 
        article: Dict[str, Any], 
        country: str,
        category: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """Find a similar existing topic for the given article using semantic similarity.
        
        Args:
            article: The article to find a topic for.
            country: Country code.
            category: Optional category to filter topics by.
            
        Returns:
            A matching topic document or None if not found.
        """
        # Create a new MongoDBManager instance
        mongodb_manager = MongoDBManager()
        await mongodb_manager.connect_to_mongodb()
        
        try:
            if 'embedding' not in article or not article['embedding']:
                return None
                
            article_embedding = np.array(article['embedding']).reshape(1, -1)
            
            # Determine the article's category if not provided
            if category is None:
                category = self._categorize_article(article)
                if category == 'Irrelevante':
                    category = None  # Don't filter by category for irrelevant articles
            
            async with mongodb_manager.get_db() as db:
                # Build the base query
                query = {
                    "country_focus": country.upper(),
                    "is_active": True,
                    "embedding": {"$exists": True, "$ne": None}
                }
                
                # Add category filter if provided
                if category:
                    query["category"] = category
                
                # Search for recent topics (last 30 days by default)
                min_date = datetime.utcnow() - timedelta(days=30)
                query["created_at"] = {"$gte": min_date}
                
                logger.info(f"Buscando tópicos{ ' na categoria ' + category if category else '' } desde {min_date}")
                
                # Find potential matching topics
                recent_topics = await db.topics.find(query).to_list(length=200)  # Increased limit
                
                if not recent_topics:
                    logger.debug("Nenhum tópico recente encontrado para comparação")
                    return None
                
                # Prepare topic embeddings and metadata
                topic_embeddings = []
                valid_topics = []
                
                for topic in recent_topics:
                    if 'embedding' in topic and topic['embedding'] is not None:
                        try:
                            # Ensure the embedding is a 1D numpy array
                            emb = np.array(topic['embedding']).flatten()
                            if emb.shape[0] > 0:  # Check if the embedding is not empty
                                topic_embeddings.append(emb)
                                valid_topics.append(topic)
                        except Exception as e:
                            logger.warning(f"Erro ao processar embedding do tópico {topic.get('_id')}: {str(e)}")
                            continue
                
                if not topic_embeddings:
                    logger.debug("Nenhum embedding de tópico válido encontrado")
                    return None
                
                logger.debug(f"Comparando com {len(valid_topics)} tópicos válidos")
                
                # Convert to 2D numpy array (n_samples, n_features)
                topic_embeddings = np.vstack(topic_embeddings)
                
                # Ensure the article embedding has the same dimensionality
                article_embedding = article_embedding.reshape(1, -1)
                
                # Handle dimensionality mismatch
                if article_embedding.shape[1] > topic_embeddings.shape[1]:
                    # Truncate article embedding if it's larger
                    article_embedding = article_embedding[:, :topic_embeddings.shape[1]]
                elif article_embedding.shape[1] < topic_embeddings.shape[1]:
                    # Pad article embedding with zeros if it's smaller
                    padding = np.zeros((1, topic_embeddings.shape[1] - article_embedding.shape[1]))
                    article_embedding = np.hstack([article_embedding, padding])
                
                # Calculate cosine similarity
                try:
                    similarities = cosine_similarity(article_embedding, topic_embeddings)[0]
                except Exception as e:
                    logger.error(f"Erro ao calcular similaridade: {str(e)}")
                    return None
                
                # Find the most similar topic above the threshold
                max_similarity_idx = np.argmax(similarities)
                max_similarity = similarities[max_similarity_idx]
                
                # Adjust similarity threshold based on category
                threshold = self.similarity_threshold
                if category in ['Saúde', 'Pesquisa']:
                    # Be more strict with health and research topics
                    threshold = min(0.5, threshold * 1.1)
                elif category in ['Família', 'Inclusão']:
                    # Be more lenient with family and inclusion topics
                    threshold = max(0.3, threshold * 0.9)
                
                logger.debug(f"Similaridade máxima encontrada: {max_similarity:.4f} (limiar: {threshold:.4f})")
                
                # If similarity is above threshold, return the topic
                if max_similarity > threshold:
                    similar_topic = valid_topics[max_similarity_idx]
                    logger.info(
                        f"Tópico similar encontrado: '{similar_topic.get('title', 'Sem título')[:50]}' "
                        f"(similaridade: {max_similarity:.2f}, categoria: {similar_topic.get('category', 'N/A')})"
                    )
                    return similar_topic
                
                logger.debug(f"Nenhum tópico similar encontrado acima do limiar de {threshold:.2f}")
                return None
                
        except Exception as e:
            logger.error(f"Error finding similar topic: {str(e)}", exc_info=True)
            return None
            
        finally:
            # Ensure the connection is closed
            if 'mongodb_manager' in locals():
                await mongodb_manager.close_mongodb_connection()
    
    async def _update_existing_topic(
        self, 
        topic: Dict[str, Any], 
        articles: List[Dict[str, Any]]
    ) -> None:
        """Update an existing topic with new articles.
        
        Args:
            topic: The existing topic document.
            articles: New articles to add to the topic (can be dicts or ObjectIds).
            
        Raises:
            ValueError: If the topic document is invalid or articles list is empty.
        """
        if not topic or '_id' not in topic:
            logger.error("Cannot update topic: invalid topic document")
            return
            
        if not articles:
            logger.warning("No articles provided to update topic")
            return
            
        logger.info(f"Updating topic {topic.get('_id')} with {len(articles)} new articles")
        
        # Create a new MongoDBManager instance
        mongodb_manager = MongoDBManager()
        await mongodb_manager.connect_to_mongodb()
        
        try:
            async with mongodb_manager.get_db() as db:
                # Get existing article IDs and URLs to avoid duplicates
                existing_articles = topic.get('articles', [])
                
                # Se articles for uma lista de IDs, buscar os documentos completos
                if existing_articles and (isinstance(existing_articles[0], str) or hasattr(existing_articles[0], 'get') is False):
                    # Converter ObjectIds para strings para a consulta
                    article_ids = [ObjectId(id_) if isinstance(id_, str) else id_ for id_ in existing_articles]
                    existing_articles = await db.news.find({
                        "_id": {"$in": article_ids}
                    }).to_list(length=100)
                
                # Extrair URLs e IDs dos artigos existentes
                existing_article_urls = set()
                existing_article_ids_set = set()
                
                for article in existing_articles:
                    if hasattr(article, 'get'):  # Se for um dicionário
                        url = article.get('original_url', '')
                        article_id = str(article.get('_id', ''))
                    else:  # Se for um ObjectId ou string
                        url = ''
                        article_id = str(article)
                    
                    if url:
                        existing_article_urls.add(url)
                    if article_id:
                        existing_article_ids_set.add(article_id)
                
                # Filtrar novos artigos que ainda não estão no tópico
                new_articles = []
                for article in articles:
                    if not article:
                        continue
                        
                    # Obter URL e ID do artigo, seja ele um dicionário ou ObjectId
                    if hasattr(article, 'get'):  # Se for um dicionário
                        article_url = article.get('original_url', '')
                        article_id = str(article.get('_id', ''))
                    else:  # Se for um ObjectId ou string
                        article_url = ''
                        article_id = str(article)
                    
                    # Verificar se o artigo já está no tópico
                    if (not article_url or article_url not in existing_article_urls) and \
                       (not article_id or article_id not in existing_article_ids_set):
                        new_articles.append(article)
                
                if not new_articles:
                    logger.info("No new articles to add to the topic")
                    return
                
                logger.info(f"Adding {len(new_articles)} new articles to topic {topic['_id']}")
                
                # Obter IDs dos artigos existentes no tópico
                existing_article_ids = set()
                for item in topic.get('articles', []):
                    if hasattr(item, 'get'):  # Se for um dicionário
                        article_id = str(item.get('_id', ''))
                    else:  # Se for um ObjectId ou string
                        article_id = str(item)
                    if article_id:
                        existing_article_ids.add(article_id)
                
                # Obter IDs dos novos artigos que ainda não estão no tópico
                new_article_ids = []
                for article in new_articles:
                    if hasattr(article, 'get'):  # Se for um dicionário
                        article_id = str(article.get('_id', ''))
                    else:  # Se for um ObjectId ou string
                        article_id = str(article)
                        
                    if article_id and article_id not in existing_article_ids:
                        new_article_ids.append(article_id)
                
                if not new_article_ids:
                    logger.info("No new article IDs to add to topic")
                    return
                    
                # Atualizar a lista de IDs de artigos no tópico
                updated_article_ids = list(existing_article_ids.union(new_article_ids))
                
                # Obter fontes únicas dos novos artigos
                new_sources = set()
                for article in new_articles:
                    if hasattr(article, 'get'):  # Se for um dicionário
                        source = article.get('source_name')
                        if source:
                            new_sources.add(source)
                
                # Preparar operação de atualização
                update_data = {
                    "$set": {
                        "articles": updated_article_ids,
                        "updated_at": datetime.utcnow(),
                        "article_count": len(updated_article_ids),
                        "last_updated": datetime.utcnow()
                    }
                }
                
                if new_sources:
                    update_data["$addToSet"] = {
                        "sources": {"$each": list(new_sources)}
                    }
                
                # Update topic
                result = await db.topics.update_one(
                    {"_id": topic['_id']},
                    update_data
                )
                
                if result.modified_count == 0:
                    logger.warning(f"No changes made to topic {topic['_id']}")
                
                # Update articles to mark them as processed
                article_ids = [article["_id"] for article in articles if "_id" in article]
                if article_ids:
                    update_result = await db.news.update_many(
                        {"_id": {"$in": article_ids}},
                        {"$set": {"in_topic": True, "topic_id": topic['_id']}}
                    )
                    logger.info(f"Updated {update_result.modified_count} articles with topic ID {topic['_id']}")
                
                logger.info(f"Successfully updated topic {topic['_id']}")
                
        except Exception as e:
            logger.error(f"Error updating topic: {str(e)}", exc_info=True)
            raise
            
        finally:
            # Ensure the connection is closed
            if 'mongodb_manager' in locals():
                await mongodb_manager.close_mongodb_connection()
    
    async def _create_new_topic(
        self, 
        main_article: Dict[str, Any], 
        articles: List[Dict[str, Any]],
        country: str,
        category: str = 'Geral'
    ) -> None:
        """Create a new topic from a cluster of articles.
        
        Args:
            main_article: The main article for the topic.
            articles: All articles in the topic.
            country: Country code.
            category: The main category for this topic.
            
        Raises:
            ValueError: If articles list is empty or main_article is not in articles.
        """
        if not articles:
            raise ValueError("Cannot create topic: no articles provided")
            
        # Ensure main_article is in the articles list
        if not any(str(a.get('_id')) == str(main_article.get('_id')) for a in articles):
            articles.insert(0, main_article)
            
        logger.info(f"Creating new topic in category '{category}' with main article: {main_article.get('title')}")
        
        # Create a new MongoDBManager instance
        mongodb_manager = MongoDBManager()
        await mongodb_manager.connect_to_mongodb()
        
        try:
            async with mongodb_manager.get_db() as db:
                # Check if any of these articles already belong to another topic
                article_ids = [article.get('_id') for article in articles if article.get('_id')]
                
                # Find any existing topics that already contain these articles
                existing_topics = await db.topics.find({
                    "articles": {"$in": article_ids},
                    "is_active": True
                }).to_list(length=10)
                
                # If we found existing topics, add to the most similar one instead of creating a new one
                if existing_topics:
                    logger.info(f"Found {len(existing_topics)} existing topics with these articles")
                    # For now, just pick the first one - we'll improve this later
                    await self._update_existing_topic(existing_topics[0], articles)
                    return
                
                # Extract and clean text from articles
                combined_content = []
                keywords = set()
                
                for article in articles:
                    # Get the best available title
                    title = next((
                        article.get(field, '') 
                        for field in ['extracted_title', 'serpapi_title', 'title'] 
                        if field in article and article[field]
                    ), 'Sem título')
                    
                    # Get the best available summary/content
                    summary = next((
                        article.get(field, '') 
                        for field in ['summary', 'serpapi_snippet', 'content'] 
                        if field in article and article[field]
                    ), '')
                    
                    if title or summary:
                        combined_content.append(f"{title}\n{summary}")
                    
                    # Extract keywords from title and summary
                    text = f"{title} {summary}".lower()
                    words = set(re.findall(r'\b\w{4,}\b', text))  # Words with 4+ characters
                    keywords.update(words)
                
                # Remove common stopwords from keywords
                stopwords = {'sobre', 'para', 'como', 'mais', 'muito', 'pode', 'ser', 'está', 'são', 'com', 'que', 'uma', 'por', 'não', 'seu', 'sua', 'seja'}
                keywords = [kw for kw in keywords if kw not in stopwords][:20]  # Limit to top 20 keywords
                
                # Generate a combined text for summarization
                combined_text = "\n\n".join(combined_content)
                
                # Generate a summary using AI if possible, otherwise use a simple extract
                topic_summary = ""
                if combined_text:
                    try:
                        # Try to generate a concise summary with AI
                        prompt = f"Resuma em 1-2 frases o tópico principal destas notícias sobre {category}:\n\n{combined_text[:3000]}"
                        topic_summary = await ai_processor.summarize_text(prompt, max_length=200)
                        logger.debug(f"Generated topic summary: {topic_summary[:100]}...")
                    except Exception as e:
                        logger.error(f"Error generating topic summary: {str(e)}", exc_info=True)
                        # Fallback: Use the beginning of the main article's content
                        topic_summary = next((
                            main_article.get(field, '') 
                            for field in ['summary', 'serpapi_snippet', 'content'] 
                            if field in main_article and main_article[field]
                        ), '')[:200]
                
                # Generate a descriptive title for the topic
                title = main_article.get('extracted_title') or main_article.get('serpapi_title') or main_article.get('title', 'Sem título')
                
                # If we have multiple articles, try to find a common theme for the title
                if len(articles) > 1 and len(combined_text) > 0:
                    try:
                        # Use AI to generate a more descriptive title if possible
                        prompt = f"Crie um título curto e descritivo para um tópico sobre {category} que abranja estas notícias:\n\n{combined_text[:2000]}"
                        title = await ai_processor.summarize_text(prompt, max_length=100)
                        logger.debug(f"Generated topic title: {title}")
                    except Exception as e:
                        logger.error(f"Error generating topic title: {str(e)}", exc_info=True)
                
                # Create topic document with enhanced metadata
                topic_doc = {
                    "name": title,
                    "title": title,
                    "summary": topic_summary,
                    "category": category,
                    "keywords": keywords,
                    "articles": article_ids,
                    "article_count": len(articles),
                    "sources": list({a.get('source_name', '') for a in articles if a.get('source_name')}),
                    "country_focus": country.upper(),
                    "created_at": datetime.utcnow(),
                    "updated_at": datetime.utcnow(),
                    "is_active": True,
                    "embedding": main_article.get('embedding'),
                    "main_article_id": str(main_article.get('_id', '')),
                    "first_seen": self._get_article_date(main_article),
                    "last_updated": datetime.utcnow(),
                    "language": "pt"  # Assuming Portuguese content for now
                }
                
                # Save topic
                result = await db.topics.insert_one(topic_doc)
                topic_id = result.inserted_id
                
                # Update articles to mark them as processed and link to this topic
                await db.news.update_many(
                    {"_id": {"$in": article_ids}},
                    {
                        "$set": {
                            "in_topic": True, 
                            "topic_id": topic_id,
                            "topic_category": category,
                            "topic_updated_at": datetime.utcnow()
                        }
                    }
                )
                
                logger.info(f"Created new topic '{title}' (ID: {topic_id}) with {len(articles)} articles in category '{category}'")
                
        except Exception as e:
            logger.error(f"Error creating new topic: {str(e)}", exc_info=True)
            raise
            
        finally:
            # Ensure the connection is closed
            if 'mongodb_manager' in locals():
                await mongodb_manager.close_mongodb_connection()

    async def _merge_similar_topics(
        self, 
        topic1: Dict[str, Any], 
        topic2: Dict[str, Any]
    ) -> None:
        """Merge two similar topics into one.
        
        Args:
            topic1: The first topic document.
            topic2: The second topic document.
            
        Raises:
            ValueError: If either topic document is invalid.
        """
        if not topic1 or not isinstance(topic1, dict) or '_id' not in topic1:
            logger.error("Cannot merge topics: invalid topic1 document")
            return
            
        if not topic2 or not isinstance(topic2, dict) or '_id' not in topic2:
            logger.error("Cannot merge topics: invalid topic2 document")
            return
            
        topic1_id = topic1.get('_id')
        topic2_id = topic2.get('_id')
        
        if not topic1_id or not topic2_id:
            logger.error("Cannot merge topics: missing topic IDs")
            return
            
        # Ensure we're not trying to merge a topic with itself
        if topic1_id == topic2_id:
            logger.warning(f"Cannot merge topic {topic1_id} with itself")
            return
        
        logger.info(f"Merging topic {topic2_id} into {topic1_id}")
        
        # Create a new MongoDBManager instance
        mongodb_manager = MongoDBManager()
        await mongodb_manager.connect_to_mongodb()
        
        try:
            async with mongodb_manager.get_db() as db:
                # Get fresh copies of both topics from the database
                fresh_topic1 = await db.topics.find_one({"_id": topic1_id})
                fresh_topic2 = await db.topics.find_one({"_id": topic2_id})
                
                if not fresh_topic1 or not fresh_topic2:
                    logger.error("One or both topics not found in database")
                    return
                
                # Get existing article IDs
                topic1_article_ids = set()
                if 'articles' in fresh_topic1:
                    for id_ in fresh_topic1['articles']:
                        try:
                            topic1_article_ids.add(str(id_))
                        except Exception as e:
                            logger.warning(f"Invalid article ID in topic1: {id_}, error: {str(e)}")
                
                topic2_article_ids = set()
                if 'articles' in fresh_topic2:
                    for id_ in fresh_topic2['articles']:
                        try:
                            topic2_article_ids.add(str(id_))
                        except Exception as e:
                            logger.warning(f"Invalid article ID in topic2: {id_}, error: {str(e)}")
                
                # Get new article IDs that aren't already in topic1
                new_article_ids = []
                for article_id in topic2_article_ids:
                    if article_id and article_id not in topic1_article_ids:
                        new_article_ids.append(article_id)
                
                if not new_article_ids:
                    logger.info("No new articles to add to topic")
                    # Mark topic2 as inactive since it's a subset of topic1
                    await db.topics.update_one(
                        {"_id": topic2_id},
                        {
                            "$set": {
                                "is_active": False,
                                "merged_into": topic1_id,
                                "updated_at": datetime.utcnow()
                            }
                        }
                    )
                    return
                
                # Get unique source names from new articles
                new_sources = set()
                valid_article_ids = []
                
                for article_id in new_article_ids:
                    try:
                        # Try to convert to ObjectId if it's not already one
                        article_oid = ObjectId(article_id) if not isinstance(article_id, ObjectId) else article_id
                        article = await db.news.find_one({"_id": article_oid})
                        if article:
                            valid_article_ids.append(article_oid)
                            if 'source_name' in article:
                                new_sources.add(article['source_name'])
                    except Exception as e:
                        logger.warning(f"Error processing article {article_id}: {str(e)}")
                
                if not valid_article_ids:
                    logger.warning("No valid articles to merge")
                    return
                
                # Start a session for transaction support
                async with await mongodb_manager.client.start_session() as session:
                    async with session.start_transaction():
                        # Prepare update operations
                        update_data = {
                            "$addToSet": {
                                "articles": {"$each": valid_article_ids}
                            },
                            "$set": {
                                "updated_at": datetime.utcnow(),
                                "last_updated": datetime.utcnow()
                            },
                            "$inc": {"article_count": len(valid_article_ids)}
                        }
                        
                        if new_sources:
                            update_data["$addToSet"]["sources"] = {"$each": list(new_sources)}
                        
                        # Update topic1 with new articles
                        result = await db.topics.update_one(
                            {"_id": topic1_id},
                            update_data,
                            session=session
                        )
                        
                        if result.modified_count == 0:
                            logger.warning(f"No changes made to topic {topic1_id}")
                        
                        # Update articles to point to topic1
                        update_result = await db.news.update_many(
                            {"_id": {"$in": valid_article_ids}},
                            {
                                "$set": {
                                    "in_topic": True,
                                    "topic_id": topic1_id,
                                    "topic_updated_at": datetime.utcnow()
                                }
                            },
                            session=session
                        )
                        
                        logger.info(f"Updated {update_result.modified_count} articles to point to topic {topic1_id}")
                        
                        # Mark topic2 as inactive and set merged_into reference
                        await db.topics.update_one(
                            {"_id": topic2_id},
                            {
                                "$set": {
                                    "is_active": False,
                                    "merged_into": topic1_id,
                                    "merged_at": datetime.utcnow(),
                                    "updated_at": datetime.utcnow()
                                }
                            },
                            session=session
                        )
                        
                        logger.info(f"Marked topic {topic2_id} as inactive and merged into {topic1_id}")
                
                logger.info(f"Successfully merged topic {topic2_id} into {topic1_id}")
                    
        except Exception as e:
            logger.error(f"Error merging similar topics: {str(e)}", exc_info=True)
            raise
            
        finally:
            # Ensure the connection is closed
            if 'mongodb_manager' in locals():
                await mongodb_manager.close_mongodb_connection()

# Create a singleton instance
topic_cluster = TopicCluster()
